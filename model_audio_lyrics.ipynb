{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lyric_tokens(tokens):\n",
    "    #remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    #remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    #filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    #filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokens_to_line(tokens, vocab):\n",
    "    # clean doc\n",
    "    tokens = clean_lyric_tokens(tokens)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def baseline_model():\n",
    "    audio_model = keras.Sequential([\n",
    "        layers.Dense(128, input_dim=6, activation=tf.nn.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "    ])\n",
    "    \n",
    "    lyrics_model = keras.Sequential([\n",
    "        layers.Dense(64, input_shape=(4563,), activation=tf.nn.relu),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dropout(0.5),\n",
    "    ])\n",
    "    \n",
    "    merged = layers.Add()([audio_model.output, lyrics_model.output])\n",
    "    merged = layers.Dense(64, activation=tf.nn.relu)(merged)\n",
    "    merged = layers.Dense(5, activation=tf.nn.softmax)(merged)\n",
    "    \n",
    "    model = keras.Model([audio_model.input, lyrics_model.input], merged)\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18437484, -0.25081067, -1.41576069, -1.37910795,  0.02406681,\n",
       "        -0.85059275],\n",
       "       [ 0.9760505 , -0.03265142, -0.25196577,  0.03337197,  0.03269329,\n",
       "        -0.11590503],\n",
       "       [ 1.63194309, -0.54471966, -0.10363897, -0.59157166,  0.42964577,\n",
       "         0.42098216],\n",
       "       ...,\n",
       "       [-0.39880127, -0.85494817, -0.69314293,  0.16120665, -1.35996345,\n",
       "        -1.38182849],\n",
       "       [ 1.17155695, -0.6771302 , -0.99740304, -1.07281792, -0.98294922,\n",
       "         0.72373808],\n",
       "       [ 1.51211656, -0.73954799, -0.63989741, -1.52373842,  0.76577652,\n",
       "         0.36446771]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'dataset.csv'\n",
    "column_names = ['ID', 'Danceability', 'Acousticness', 'Energy', 'Loudness', 'Tempo', 'Valence', 'Category']\n",
    "\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = \"?\", comment='\\t', skipinitialspace=True)\n",
    "dataset = raw_dataset.copy()\n",
    "\n",
    "dataset.Category = pd.Categorical(dataset.Category)\n",
    "dataset['Label'] = dataset.Category.cat.codes\n",
    "\n",
    "model_variables = ['Danceability', 'Acousticness', 'Energy', 'Loudness', 'Tempo', 'Valence', 'Label']\n",
    "\n",
    "dataset_relevant = dataset[model_variables]\n",
    "dataset_relevant_encoded = pd.get_dummies(dataset_relevant)\n",
    "\n",
    "training_features = dataset_relevant_encoded.drop(['Label'], axis=1)\n",
    "training_target = dataset_relevant_encoded['Label']\n",
    "\n",
    "std = StandardScaler()\n",
    "audio_dataset_bal = std.fit_transform(training_features)\n",
    "audio_dataset_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 2.53212838, 1.39257974, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 2.53212838, 2.35784246, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 3.14847665, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ''\n",
    "with open(\"vocab.txt\") as f:\n",
    "    vocab = f.read()\n",
    "\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "lyrics_dataset = {}\n",
    "with open(\"lyrics_dataset.txt\") as f:\n",
    "    lyrics_dataset = dict(x.rstrip().split(None, 1) for x in f)\n",
    "\n",
    "lines = []\n",
    "labels = []\n",
    "for id in lyrics_dataset.keys():\n",
    "    lyrics_dataset[id] = eval(lyrics_dataset[id])\n",
    "    line = tokens_to_line(lyrics_dataset[id][0], vocab)\n",
    "    lines.append(line)\n",
    "    labels.append(lyrics_dataset[id][1])\n",
    "\n",
    "labels = np.array(labels)\n",
    "categories, target_label = np.unique(labels, return_inverse=True)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "lyrics_features = tokenizer.texts_to_matrix(lines, mode='tfidf')\n",
    "\n",
    "lyrics_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/harryroxas/venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1422: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dense_1_input (InputLayer)      (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          896         dense_1_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3_input (InputLayer)      (None, 4563)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           292096      dense_3_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           8256        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64)           256         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64)           0           dropout_2[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5)            325         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 310,661\n",
      "Trainable params: 310,277\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "estimator = baseline_model()\n",
    "kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=7)\n",
    "\n",
    "estimator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.506250\n",
      "[[  1  11  30   1  63]\n",
      " [  1  79  31   1  62]\n",
      " [  8  31 231   7 183]\n",
      " [  2   4  18   2  51]\n",
      " [ 15  32 146  14 416]]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kfold.split(audio_dataset_bal, target_label):\n",
    "    X_tr1, X_tes1 = audio_dataset_bal[train_index], audio_dataset_bal[test_index]\n",
    "    X_tr2, X_tes2 = lyrics_features[train_index], lyrics_features[test_index]\n",
    "    y_tr, y_tes = target_label[train_index], target_label[test_index]\n",
    "    estimator.fit([X_tr1, X_tr2], y_tr, epochs=100, batch_size=128, verbose=0) \n",
    "\n",
    "    y_pred=estimator.predict([X_tes1, X_tes2])\n",
    "    acc = accuracy_score(y_tes, np.argmax(y_pred, axis=1))\n",
    "    cnf_matrix = confusion_matrix(y_tes, np.argmax(y_pred, axis=1))\n",
    "    print(\"Accuracy:  %f\" % acc)\n",
    "    print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
